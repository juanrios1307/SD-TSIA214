{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP : Sentiment analysis on IMDB movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "1. Implement a simple way to represent text data - Bag of words\n",
    "2. Implement a basic statistical learning model - Bayesian Naive\n",
    "3. Use these representations and this model for a sentiment analysis task.\n",
    "4. Experiment with various way to reduce the vocabulary size and look at the distribution obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary dependancies\n",
    "\n",
    "We will need the following packages:\n",
    "- The Machine Learning API Scikit-learn : http://scikit-learn.org/stable/install.html\n",
    "- The Natural Language Toolkit : http://www.nltk.org/install.html\n",
    "\n",
    "Both are available with Anaconda: https://anaconda.org/anaconda/nltk and https://anaconda.org/anaconda/scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import re \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "We retrieve the textual data in the variable *texts*.\n",
    "\n",
    "The labels are retrieved in the variable $y$ - it contains *len(texts)* of them: $0$ indicates that the corresponding review is negative while $1$ indicates that it is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "# We get the files from the path: ./aclImdb/train/neg for negative reviews, and ./aclImdb/train/pos for positive reviews\n",
    "train_filenames_neg = sorted(glob(op.join('.', 'neg', '*.txt')))\n",
    "train_filenames_pos = sorted(glob(op.join('.', 'pos', '*.txt')))\n",
    "\n",
    "# Each files contains a review that consists in one line of text: we put this string in two lists, that we concatenate\n",
    "train_texts_neg = [open(f, encoding=\"utf8\").read() for f in train_filenames_neg]\n",
    "train_texts_pos = [open(f, encoding=\"utf8\").read() for f in train_filenames_pos]\n",
    "train_texts = train_texts_neg + train_texts_pos\n",
    "\n",
    "# The first half of the elements of the list are string of negative reviews, and the second half positive ones\n",
    "# We create the labels, as an array of [1,len(texts)], filled with 1, and change the first half to 0\n",
    "train_labels = np.ones(len(train_texts), dtype=int)\n",
    "train_labels[:len(train_texts_neg)] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(\"./aclImdb/train/neg/0_3.txt\", encoding=\"utf8\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this lab, the impact of our choice of representations upon our results will also depend on the quantity of data we use:** try to see how changing the parameter ```k``` affects our results !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This number of documents may be high for most computers: we can select a fraction of them (here, one in k)\n",
    "# Use an even number to keep the same number of positive and negative reviews\n",
    "k = 10\n",
    "train_texts_reduced = train_texts[0::k]\n",
    "train_labels_reduced = train_labels[0::k]\n",
    "\n",
    "print('Number of documents:', len(train_texts_reduced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a function from sklearn, ```train_test_split```, to separate data into training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_splt, val_texts, train_labels_splt, val_labels = train_test_split(train_texts_reduced, train_labels_reduced, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I - Adapted representation of documents\n",
    "\n",
    "Our statistical model, like most models applied to textual data, uses counts of word occurrences in a document. Thus, a very convenient way to represent a document is to use a Bag-of-Words (BoW) vector, containing the counts of each word (regardless of their order of occurrence) in the document. \n",
    "\n",
    "If we consider the set of all the words appearing in our $T$ training documents, which we note $V$ (Vocabulary), we can create **an index**, which is a bijection associating to each $w$ word an integer, which will be its position in $V$. \n",
    "\n",
    "Thus, for a document extracted from a set of documents containing $|V|$ different words, a BoW representation will be a vector of size $|V|$, whose value at the index of a word $w$ will be its number of occurrences in the document. \n",
    "\n",
    "We can use the **CountVectorizer** class from scikit-learn to obtain these representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['I walked down down the boulevard',\n",
    "          'I walked down the avenue',\n",
    "          'I ran down the boulevard',\n",
    "          'I walk down the city',\n",
    "          'I walk down the the avenue']\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "Bow = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "Bow.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display the list containing the words ordered according to their index (Note that words of 2 characters or less are not counted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the text: get the BoW representations ##\n",
    "\n",
    "The first thing to do is to turn the review from a string into a list of words. The simplest method is to divide the string according to spaces with the command:\n",
    "``text.split()``\n",
    "\n",
    "But we must also be careful to remove special characters that may not have been cleaned up (such as HTML tags if the data was obtained from web pages). Since we're going to count words, we'll have to build a list of tokens appearing in our data. In our case, we'd like to reduce this list and make it uniform (ignore capitalization, punctuation, and the shortest words). \n",
    "We will therefore use a function adapted to our needs - but this is a job that we generally don't need to do ourselves, since there are many tools already adapted to most situations. \n",
    "For text cleansing, there are many scripts, based on different tools (regular expressions, for example) that allow you to prepare data. The division of the text into words and the management of punctuation is handled in a step called *tokenization*; if needed, a python package like NLTK contains many different *tokenizers*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We might want to clean the file with various strategies:\n",
    "def clean_and_tokenize(text):\n",
    "    \"\"\"\n",
    "    Cleaning a document with:\n",
    "        - Lowercase        \n",
    "        - Removing numbers with regular expressions\n",
    "        - Removing punctuation with regular expressions\n",
    "        - Removing other artifacts\n",
    "    And separate the document into words by simply splitting at spaces\n",
    "    Params:\n",
    "        text (string): a sentence or a document\n",
    "    Returns:\n",
    "        tokens (list of strings): the list of tokens (word units) forming the document\n",
    "    \"\"\"        \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"[0-9]+\", \"\", text)\n",
    "    # Remove punctuation\n",
    "    REMOVE_PUNCT = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "    text = REMOVE_PUNCT.sub(\"\", text)\n",
    "    # Remove small words (1 and 2 characters)\n",
    "    text = re.sub(r\"\\b\\w{1,2}\\b\", \"\", text)\n",
    "    # Remove HTML artifacts specific to the corpus we're going to work with\n",
    "    REPLACE_HTML = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "    text = REPLACE_HTML.sub(\" \", text)\n",
    "    \n",
    "    tokens = text.split()        \n",
    "    return tokens\n",
    "\n",
    "# Or we might want to use an already-implemented tool. The NLTK package has a lot of very useful text processing tools, among them various tokenizers\n",
    "# Careful, NLTK was the first well-documented NLP package, but it might be outdated for some uses. Check the documentation !\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "corpus_raw = \"I walked down down the boulevard. I walked down the avenue. I ran down the boulevard. I walk down the city. I walk down the the avenue.\"\n",
    "print(clean_and_tokenize(corpus_raw))\n",
    "print(word_tokenize(corpus_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function takes as input a list of documents (each in the form of a string) and returns, as in the example using ``CountVectorizer``:\n",
    "- A vocabulary that associates, to each word encountered, an index\n",
    "- A matrix, with rows representing documents and columns representing words indexed by the vocabulary. In position $(i,j)$, one should have the number of occurrences of the word $j$ in the document $i$.\n",
    "\n",
    "The vocabulary, which was in the form of a *list* in the previous example, can be returned in the form of a *dictionary* whose keys are the words and values are the indices. Since the vocabulary lists the words in the corpus without worrying about their number of occurrences, it can be built up using a set (in python).\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(texts):\n",
    "    \"\"\"Vectorize text : return count of each word in the text snippets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : list of str\n",
    "        The texts\n",
    "    Returns\n",
    "    -------\n",
    "    vocabulary : dict\n",
    "        A dictionary that points to an index in counts for each word.\n",
    "    counts : ndarray, shape (n_samples, n_features)\n",
    "        The counts of each word in each text.\n",
    "    \"\"\"\n",
    "    vocabulary = []\n",
    "    for text in texts:\n",
    "        vocabulary += clean_and_tokenize(text)\n",
    "    \n",
    "    vocabulary = set(vocabulary)\n",
    "    vocabulary = dict(zip(list(vocabulary), range(len(vocabulary))))    \n",
    "\n",
    "    counts = np.zeros((len(texts), len(vocabulary)), dtype=int)\n",
    "    for i, text in enumerate(texts):\n",
    "        for word in clean_and_tokenize(text):\n",
    "            counts[i, vocabulary[word]] += 1\n",
    "    \n",
    "    return vocabulary, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Voc, X = count_words(corpus)\n",
    "print(Voc)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we want to represent text that was not available when building the vocabulary, we will not be able to represent **new words** ! Let's take a look at how CountVectorizer does it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_corpus = ['I walked up the street']\n",
    "Bow = vectorizer.transform(val_corpus)\n",
    "Bow.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the ```count_words``` function to be able to deal with new documents when given a previously obtained vocabulary ! \n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(texts, voc = None):\n",
    "    \"\"\"Vectorize text : return count of each word in the text snippets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : list of str\n",
    "        The texts\n",
    "    voc : dict\n",
    "        A dictionary that points to an index in counts for each word - output by a previous use of the function\n",
    "        If not None, use this one\n",
    "    Returns\n",
    "    -------\n",
    "    vocabulary : dict\n",
    "        A dictionary that points to an index in counts for each word.\n",
    "    counts : ndarray, shape (n_samples, n_features)\n",
    "        The counts of each word in each text.\n",
    "    \"\"\"\n",
    "    if voc == None:\n",
    "\n",
    "        vocabulary = []\n",
    "        for text in texts:\n",
    "            vocabulary += clean_and_tokenize(text)\n",
    "        \n",
    "        vocabulary = set(vocabulary)\n",
    "        vocabulary = dict(zip(list(vocabulary), range(len(vocabulary))))    \n",
    "\n",
    "        counts = np.zeros((len(texts), len(vocabulary)), dtype=int)\n",
    "        for i, text in enumerate(texts):\n",
    "            for word in clean_and_tokenize(text):\n",
    "                counts[i, vocabulary[word]] += 1\n",
    "        \n",
    "    else:\n",
    "\n",
    "        vocabulary = voc\n",
    "        counts = np.zeros((len(texts), len(vocabulary)), dtype=int)\n",
    "        for i, text in enumerate(texts):\n",
    "            for word in clean_and_tokenize(text):\n",
    "                if word in vocabulary:\n",
    "                    counts[i, vocabulary[word]] += 1\n",
    "    return vocabulary, counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-warning'>\n",
    "            Questions:</div>                       \n",
    "\n",
    "Careful: check the size that the representations are going to have (given the way they are build). What does this imply for the memory use ? What ```CountVectorizer``` argument allows to avoid the issue ?\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc, train_bow = count_words(train_texts_splt)\n",
    "print(train_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, val_bow = count_words(val_texts, voc)\n",
    "print(val_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same with a CountVectorizer: create and fit the vectorizer to the training data\n",
    "vectorizer = CountVectorizer()\n",
    "train_bow_aux = vectorizer.fit_transform(train_texts_splt)\n",
    "vocabulary_aux = vectorizer.get_feature_names_out()\n",
    "train_bow_aux.toarray()\n",
    "indexes = np.arange(len(vocabulary_aux)).tolist()\n",
    "vocabulary_aux = dict(zip(vocabulary_aux, indexes))\n",
    "print(train_bow_aux.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the validation data\n",
    "val_bow_aux = vectorizer.transform(val_texts)\n",
    "val_bow_aux = val_bow_aux.toarray()\n",
    "print(val_bow_aux.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II- Naive Bayesian \n",
    "\n",
    "### Main idea\n",
    "\n",
    "A movie review is in fact a list of words $s = (w_1, ..., w_N)$, and we try to find the associated class $c$ - which in our case may be $c = 0$ or $c = 1$. The objective is thus to find for each review $s$ the class $\\hat{c}$ maximizing the conditional probability **$P(c|s)$** : \n",
    "\n",
    "$$\\hat{c} = \\underset{c}{\\mathrm{argmax}}\\, P(c|s) = \\underset{c}{\\mathrm{argmax}}\\,\\frac{P(s|c)P(c)}{P(s)}$$\n",
    "\n",
    "**Hypothesis : P(s) is constant for each class** :\n",
    "\n",
    "$$\\hat{c} = \\underset{c}{\\mathrm{argmax}}\\,\\frac{P(s|c)P(c)}{P(s)} = \\underset{c}{\\mathrm{argmax}}\\,P(s|c)P(c)$$\n",
    "\n",
    "**Naive hypothesis : the variables (words) of a review are independant between themselves** : \n",
    "\n",
    "$$P(s|c) = P(w_1, ..., w_N|c)=\\Pi_{i=1..N}P(w_i|c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General view\n",
    "\n",
    "#### Training: Estimating the probabilities\n",
    "\n",
    "For each word $w$ in the vocabulary $V$, $P(w|c)$ is the number of occurrences of $w$ in all reviews of class $c$, divided by the total number of occurrences in $c$. If we note $T(w,c)$ this number of occurrences, we get:\n",
    "\n",
    "$$P(w|c) = \\text{Frequency of }w\\text{ in }c = \\frac{T(w,c)}{\\sum_{w' \\in V} T(w',c)}$$\n",
    "\n",
    "#### Test: Calculating scores\n",
    "\n",
    "To facilitate the calculations and to avoid *underflow* and approximation errors, we use the log-sum trick, and we pass the equation into log-probabilities : \n",
    "\n",
    "$$ \\hat{c} = \\underset{c}{\\mathrm{argmax}} P(c|s) = \\underset{c}{\\mathrm{argmax}} \\left[ \\mathrm{log}(P(c)) + \\sum_{i=1..N}log(P(w_i|c)) \\right] $$\n",
    "\n",
    "#### Laplace smoothing\n",
    "\n",
    "A word that does not appear in a document has a probability of zero: this will cause issues with the logarithm. So we keep a very small part of the probability mass that we redistribute with the *Laplace smoothing*: \n",
    "\n",
    "$$P(w|c) = \\frac{T(w,c) + 1}{\\sum_{w' \\in V} (T(w',c) + 1)}$$\n",
    "\n",
    "There are other smoothing methods, generally suitable for other, more complex applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detail: training\n",
    "\n",
    "The idea is to extract the number of occurrences $T(w,c)$ for each word $w$ and each class $c$, which will make it possible to calculate the matrix of conditional probabilities $\\pmb{P}$ such that: $$\\pmb{P}_{w,c} = P(w|c)$$\n",
    "\n",
    "Note that the number of occurrences $T(w,c)$ can be easily obtained from the BoW representations of all documents !\n",
    "\n",
    "#### Procedure:\n",
    "\n",
    "- Extract the vocabulary $V$ and counts $T(w,c)$ for each of the words $w$ and classes $c$, from a set of documents.\n",
    "- Calculate the a priori probabilities of the classes $P(c) = \\frac{|\\text{documents in class }c|}{|\\text{all documents}|}$\n",
    "- Calculate the conditional **smoothed** probabilities $P(w|c) = \\frac{T(w,c) + 1}{\\sum_{w' \\in V} T(w',c) + 1}$.\n",
    "\n",
    "### Detail: test\n",
    "\n",
    "We now know the conditional probabilities given by the $\\pmb{P}$ matrix. \n",
    "Now we must obtain $P(s|c)$ for the current document. This quantity is obtained using a simple calculation involving the BoW representation of the document and $\\pmb{P}$.\n",
    "\n",
    "#### Procedure:\n",
    "\n",
    "- For each of the classes $c$,\n",
    "    - $Score(c) = \\log P(c)$\n",
    "    - For each word $w$ in the document to be tested:\n",
    "        - $Score(c) += \\log P(w|c)$\n",
    "- Return $argmax_{c \\in C} Score(c)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will therefore be able to use the reviews at our disposal to **estimate the probabilities $P(w|c)$ for each word $w$ given the two classes $c$**. These reviews will allow us to learn how to evaluate the \"compatibility\" between words and classes.\n",
    "```python\n",
    "def fit(self, X, y)\n",
    "``` \n",
    "**Training**: will learn a statistical model based on the representations $X$ corresponding to the labels $y$.\n",
    "Here, $X$ contains representations obtained as the output of ```count_words```. You can complete the function using the procedure detailed above. \n",
    "\n",
    "Note: the smoothing is not necessarily done with a $1$ - it can be done with a positive value $\\alpha$, which we can implement as an argument of the class \"NB\".\n",
    "\n",
    "```python\n",
    "def predict(self, X)\n",
    "```\n",
    "**Testing**: will return the labels predicted by the model for other representations $X$.\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NB(BaseEstimator, ClassifierMixin):\n",
    "    # Les arguments de classe permettent l'héritage de classes de sklearn\n",
    "    def __init__(self, alpha=1.0):\n",
    "        # alpha is the smoothing parameter: it corresponds to line 10 of the training algorithm\n",
    "        # By default, we use alpha = 1\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        #Extract CountVectorizer to extract vocabulary\n",
    "        vectorizer = CountVectorizer()\n",
    "        X_counts = vectorizer.fit_transform(X)\n",
    "\n",
    "        #Extract vocabulary \n",
    "        self.vocabulary = vectorizer.get_feature_names_out()\n",
    "        \n",
    "        #Calculate Probabilities\n",
    "        classes, class_counts = np.unique(y, return_counts=True)\n",
    "        self.class_prob = class_counts / len(y)\n",
    "\n",
    "        #Initialize conditional probabilities P(w|c)\n",
    "        self.cond_prob = np.zeros((len(classes), len(self.vocabulary)))\n",
    "\n",
    "        for index, c in enumerate(classes):\n",
    "            X_c = X_counts[y == c]\n",
    "            \n",
    "            #Calculate T(w,c)\n",
    "            word_counts = X_c.sum(axis=0)+self.alpha\n",
    "\n",
    "            #Calculate P(w|c)\n",
    "            total_counts = word_counts.sum()\n",
    "            self.cond_prob[index] = word_counts / total_counts\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        vectorizer = CountVectorizer(vocabulary=self.vocabulary)\n",
    "        X_counts = vectorizer.fit_transform(X)\n",
    "\n",
    "        #Initialize array to store predictions\n",
    "        scores = np.zeros((X_counts.shape[0], len(self.class_prob)))\n",
    "\n",
    "        for index, (c, p_c) in enumerate(zip(self.class_prob, self.cond_prob)):\n",
    "            #Calculate log P(c) \n",
    "            scores[:, index] = np.log(c)\n",
    "\n",
    "            #Calculate log P(w|c)\n",
    "            scores[:, index] += X_counts.dot(np.log(p_c).T)\n",
    "\n",
    "        result = np.argmax(scores, axis=1)\n",
    "        return result\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return np.mean(self.predict(X) == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III - Experimentation\n",
    "\n",
    "Experiment on this model with your own representations. **Visualize** the results with the following tools, and **compare with the representations of ```CountVectorizer```/ naïve Bayes model of scikit-learn ```MultinomialNB```:**\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "nb = MultinomialNB()\n",
    "# Fit the model\n",
    "nb.fit(train_bow, train_labels_splt)\n",
    "\n",
    "#Test on validation set\n",
    "score = nb.score(val_bow, val_labels)\n",
    "print('Score on validation set:', score)\n",
    "\n",
    "#Predict on validation set\n",
    "y_pred = nb.predict(val_bow)\n",
    "\n",
    "#Confusion matrix\n",
    "cm = confusion_matrix(val_labels, y_pred)\n",
    "ConfusionMatrixDisplay(cm).plot()\n",
    "\n",
    "#Classification report\n",
    "report = classification_report(val_labels, y_pred)\n",
    "print(\"Classification report: \\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same with aux representations\n",
    "# Create the model\n",
    "nb_aux = MultinomialNB()\n",
    "# Fit the model\n",
    "nb_aux.fit(train_bow_aux, train_labels_splt)\n",
    "\n",
    "#Test on validation set\n",
    "score_aux = nb.score(val_bow_aux, val_labels)\n",
    "print('Score on validation set:', score_aux)\n",
    "\n",
    "#Predict on validation set\n",
    "y_pred_aux = nb.predict(val_bow_aux)\n",
    "\n",
    "#Confusion matrix\n",
    "cm_aux = confusion_matrix(val_labels, y_pred_aux)\n",
    "ConfusionMatrixDisplay(cm_aux).plot()\n",
    "\n",
    "#Classification report\n",
    "report_aux = classification_report(val_labels, y_pred_aux)\n",
    "print(\"Classification report: \\n\", report_aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-warning'>\n",
    "            Questions:</div>\n",
    "            \n",
    "Let us look at the *features* built by the ```vectorizer```. What seems to be the issue ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.get_feature_names()[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving representations\n",
    "\n",
    "Mainly, the arguments of the class ```vectorizer``` will allow us to easily change the way our textual data is represented. Let us try to work on our *Bag-of-words* representations:\n",
    "   \n",
    "#### Do not take into account words that are too frequent:\n",
    "\n",
    "You can use the argument ```max_df=1.0``` to change the amount of words taken into account. \n",
    "\n",
    "#### Try different granularities:\n",
    "\n",
    "Rather than just counting words, we can count sequences of words - limited in size, of course. \n",
    "We call a sequence of $n$ words a $n$-gram: let's try using 2 and 3-grams (bi- and trigrams).\n",
    "We can also try to use character sequences instead of word sequences.\n",
    "\n",
    "We will be interested in the options ```analyze='word'``` and ```ngram_range=(1, 2)``` which we'll change to alter the granularity: **obtain classification results with them**.\n",
    "\n",
    "Again: using these ways of getting more features from our text will probably have more impact if we do not have much training data to begin with ! To accelerate experiments, use the ```Pipeline``` tool from scikit-learn. \n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_base = Pipeline([\n",
    "    ('vect', CountVectorizer(max_features=30000, analyzer='word', stop_words=None)),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "pipeline_base.fit(train_texts_splt, train_labels_splt)\n",
    "val_pred = pipeline_base.predict(val_texts)\n",
    "print(classification_report(val_labels, val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variating ngram=(1,2)\n",
    "pipeline_ngram = Pipeline([\n",
    "    ('vect', CountVectorizer(max_df=0.95, max_features=30000, analyzer='word', stop_words=None, ngram_range=(1,2))),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "pipeline_ngram.fit(train_texts_splt, train_labels_splt)\n",
    "val_pred_ngram = pipeline_ngram.predict(val_texts)\n",
    "print(classification_report(val_labels, val_pred_ngram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tf-idf:\n",
    "\n",
    "This is the product of the frequency of the term (TF) and its inverse frequency in documents (IDF).\n",
    "This method is usually used to measure the importance of a term $i$ in a document $j$ relative to the rest of the corpus, from a matrix of occurrences $ words \\times documents$. Thus, for a matrix $\\mathbf{T}$ of $|V|$ terms and $D$ documents:\n",
    "$$\\text{TF}(T, w, d) = \\frac{T_{w,d}}{\\sum_{w'=1}^{|V|} T_{w',d}} $$\n",
    "\n",
    "$$\\text{IDF}(T, w) = \\log\\left(\\frac{D}{|\\{d : T_{w,d} > 0\\}|}\\right)$$\n",
    "\n",
    "$$\\text{TF-IDF}(T, w, d) = \\text{TF}(X, w, d) \\cdot \\text{IDF}(T, w)$$\n",
    "\n",
    "It can be adapted to our case by considering that the context of the second word is the document. However, TF-IDF is generally better suited to low-density matrices, since it will penalize terms that appear in a large part of the documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with this new representations using the ```TfidfTransformer``` applied on top of ```CountVectorizer```.\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bow_tfidf = TfidfTransformer().fit_transform(train_bow)\n",
    "\n",
    "# Create the model\n",
    "nb_tfidf = MultinomialNB()\n",
    "# Fit the model\n",
    "nb_tfidf.fit(train_bow_tfidf, train_labels_splt)\n",
    "\n",
    "#Test on validation set\n",
    "score_tfidf = nb_tfidf.score(val_bow, val_labels)\n",
    "print('Score on validation set:', score_tfidf)\n",
    "\n",
    "#Predict on validation set\n",
    "y_pred_tfidf = nb_tfidf.predict(val_bow)\n",
    "\n",
    "#Confusion matrix\n",
    "cm_tfidf = confusion_matrix(val_labels, y_pred_tfidf)\n",
    "ConfusionMatrixDisplay(cm_tfidf).plot()\n",
    "\n",
    "#Classification report\n",
    "report_tfidf = classification_report(val_labels, y_pred_tfidf)\n",
    "print(\"Classification report: \\n\", report_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the pipeline\n",
    "pipeline_tfidf = Pipeline([\n",
    "    ('vect', CountVectorizer(max_df=0.95, max_features=30000, analyzer='word', stop_words=None, ngram_range=(1,2))),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "pipeline_tfidf.fit(train_texts_splt, train_labels_splt)\n",
    "val_pred_tfidf = pipeline_tfidf.predict(val_texts)\n",
    "print(classification_report(val_labels, val_pred_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV - More pre-processing: getting vocabularies\n",
    "\n",
    "For more flexibility, we will implement separately a function returning the vocabulary. Here we will have to be able to control its size, either by indicating a **maximum number of words**, or a **minimum number of occurrences** to take the words into account. **We add, at the end, an \"unknown\" word that will replace all the words that do not appear in our \"limited\" vocabulary**.\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabulary(corpus, count_threshold=5, voc_threshold=10000):\n",
    "    \"\"\"    \n",
    "    Function using word counts to build a vocabulary - can be improved with a second parameter for \n",
    "    setting a frequency threshold\n",
    "    Params:\n",
    "        corpus (list of strings): corpus of sentences\n",
    "        count_threshold (int): number of occurences necessary for a word to be included in the vocabulary\n",
    "        voc_threshold (int): maximum size of the vocabulary. Use \"0\" to indicate no limit \n",
    "    Returns:\n",
    "        vocabulary (dictionary): keys: list of distinct words across the corpus\n",
    "                                 values: indexes corresponding to each word sorted by frequency   \n",
    "        vocabulary_word_counts (dictionary): keys: list of distinct words across the corpus\n",
    "                                             values: corresponding counts of words in the corpus\n",
    "    \"\"\"\n",
    "    word_counts = {}\n",
    "\n",
    "    words = []\n",
    "    \n",
    "    for sent in corpus:\n",
    "    #\n",
    "    # To complete: count word frequencies\n",
    "    #\n",
    "            text = word_tokenize(text.lower())\n",
    "            words += text\n",
    "\n",
    "    words = set(words)\n",
    "    counts_words = dict(zip(list(words), np.zeros(len(words), dtype=int)))\n",
    "\n",
    "    # Count the words\n",
    "    for text in corpus:\n",
    "        for word in word_tokenize(text.lower()):\n",
    "            counts_words[word] += 1        \n",
    "\n",
    "    filtered_word_counts = {} # Filter according to count_threhshold        \n",
    "    for word in counts_words:\n",
    "        count_aux = counts_words[word]\n",
    "        if count_aux >= count_threshold:\n",
    "            filtered_word_counts[word] = count_aux\n",
    "    \n",
    "\n",
    "    # Extract the words according to frequency\n",
    "    sorted_words = sorted(filtered_word_counts.items(), key = lambda val:(-val[1], val[0]))\n",
    "    \n",
    "    # Remove the words above voc-threshold\n",
    "    if voc_threshold == 0:\n",
    "        filtered_words = dict(sorted_words)\n",
    "    else:\n",
    "        if len(sorted_words) < voc_threshold:\n",
    "            filtered_words = dict(sorted_words)\n",
    "        else:\n",
    "            filtered_words = sorted_words[:voc_threshold]\n",
    "\n",
    "\n",
    "    # Add UNK\n",
    "    words = filtered_words\n",
    "    words['UNK'] = len(words)\n",
    "\n",
    "    # Create vocabulary from \"words\"\n",
    "    vocabulary = dict(zip(list(words), np.arange(len(filtered_words), dtype=int)))\n",
    "    \n",
    "    return vocabulary, {word: filtered_word_counts.get(word, 0) for word in vocabulary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for testing:\n",
    "\n",
    "corpus = ['I walked down down the boulevard',\n",
    "          'I walked down the avenue',\n",
    "          'I ran down the boulevard',\n",
    "          'I walk down the city',\n",
    "          'I walk down the the avenue']\n",
    "\n",
    "voc, counts = vocabulary(corpus, count_threshold = 3)\n",
    "print(voc)\n",
    "print(counts)\n",
    "\n",
    "# We expect something like this:\n",
    "# (In this example, we don't count 'UNK' unknown words, but you can if you want to. \n",
    "# How useful it may be depends on the data -> we will use the counts later with word2vec, keep that in mind) \n",
    "#  {'down': 0, 'the': 1, 'i': 2, 'UNK': 3}\n",
    "#  {'down': 6, 'the': 6, 'i': 5, 'UNK': 0}\n",
    "\n",
    "voc, counts = vocabulary(corpus)\n",
    "print(voc)\n",
    "print(counts)\n",
    "\n",
    "# We expect something like this:\n",
    "#  {'down': 0, 'the': 1, 'i': 2, 'walked': 3, 'boulevard': 4, 'avenue': 5, 'walk': 6, 'ran': 7, 'city': 8, 'UNK': 9}\n",
    "#  {'down': 6, 'the': 6, 'i': 5, 'walked': 2, 'boulevard': 2, 'avenue': 2, 'walk': 2, 'ran': 1, 'city': 1, 'UNK': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick study of the data\n",
    "\n",
    "We would like to get an idea of what's in these film reviews. So we'll get the vocabulary (in full) and represent the frequencies of the words, in order (be careful, you'll have to use a logarithmic scale): we should find back Zipf's law. This will give us an idea of the size of the vocabulary we will be able to choose: it's a matter of making a compromise between the necessary resources (size of the objects in memory) and the amount of information we can get from them (rare words can bring a lot of information, but it's difficult to learn good representations of them, because they are rare!).  \n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We would like to display the curve of word frequencies given their rank (index) in the vocabulary\n",
    "vocab, word_counts = vocabulary(train_texts)\n",
    "#\n",
    "#  To fill in !\n",
    "#\n",
    "frequency = np.array(list(word_counts.values())) / np.sum(list(word_counts.values()))\n",
    "rank = np.array(list(vocab.values()))\n",
    "\n",
    "# We can for example use the function plt.scatter()\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.title('Word counts versus rank')\n",
    "#\n",
    "#  To fill in !\n",
    "#\n",
    "plt.scatter(rank, frequency, s=1)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "# We would like to know how much of the data is represented by the 'k' most frequent words\n",
    "print('Vocabulary size: %i' % len(vocab))\n",
    "print('Part of the corpus by taking the \"x\" most frequent words ?')\n",
    "#\n",
    "#  To fill in !-\n",
    "#\n",
    "cum_frequency = np.cumsum(frequency) / np.sum(frequency) * 100\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.title('Cumulative frequency versus rank')\n",
    "plt.plot(rank, cum_frequency)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-warning'>\n",
    "            Questions:</div>\n",
    "            \n",
    "Word2vec's implementation cuts the vocabulary size by using **only words with at least 5 occurences**, by default. What vocabulary size would it give here ? Does it seem like a good compromise, looking at the graph ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Part of the corpus by taking the 1000 most frequent words: %2.3f %%' % cum_frequency[999])\n",
    "print('Part of the corpus by taking the 10000 most frequent words: %2.3f %%' % cum_frequency[9999])\n",
    "print('Part of the corpus by taking the 5000 most frequent words: %2.3f %%' % cum_frequency[4999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With pre-processing tools from NLTK\n",
    "\n",
    "We are now going to pre-process our textual data. **Note that this still will only be useful if we do not have a lot of training data to begin with !**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming \n",
    "\n",
    "Allows to go back to the root of a word: you can group different words around the same root, which facilitates generalization. Use:\n",
    "```from nltk import SnowballStemmer```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['singers', 'cat', 'generalization', 'philosophy', 'psychology', 'philosopher']\n",
    "for word in words:\n",
    "    print('word : %s ; stemmed : %s' %(word, stemmer.stem(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data transformation:**\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(texts):\n",
    "    #\n",
    "    # To complete \n",
    "    #\n",
    "    texts_stem = []\n",
    "    for text in texts:\n",
    "\n",
    "        text_stem = [stemmer.stem(word) for word in word_tokenize(text.lower())]\n",
    "        texts_stem.append(' '.join(text_stem))\n",
    "\n",
    "    \n",
    "    return texts_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part of speech tags\n",
    "\n",
    "To generalize, we can also use the Part of Speech (POS) of the words, which will allow us to filter out information that is potentially not useful to the model. We will retrieve the POS of the words using the functions:\n",
    "```pos_tag```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "pos_tag(word_tokenize(('I am Sam')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data transformation:** only keep nouns, verbs, adverbs, and adjectives (```['NN', 'VB', 'ADJ', 'RB']```) for our model.\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_filter(X, good_tags=['NN', 'VB', 'ADJ', 'RB']):\n",
    "    #\n",
    "    # To complete \n",
    "    #\n",
    "    X_pos = []\n",
    "    for text in X:\n",
    "        pos_tags = pos_tag(word_tokenize(text))\n",
    "        text_pos = [word for word, tag in pos_tags if tag in good_tags]\n",
    "        X_pos.append(' '.join(text_pos))\n",
    "        \n",
    "    return X_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application\n",
    "\n",
    "<div class='alert alert-block alert-warning'>\n",
    "            Questions:</div>\n",
    "\n",
    "Re-draw the Zipf distribution of our data **after reducing their vocabulary with these functions**. How is it affected ? How do you think it could affect results here ?         \n",
    "        \n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess texts using stemming\n",
    "train_texts_stem = stem(train_texts_splt)\n",
    "#Apply pos_tag_filter\n",
    "train_texts_pos = pos_tag_filter(train_texts_stem)\n",
    "\n",
    "#Tokenize the preprocessed texts\n",
    "voc, train_bow_pos = count_words(train_texts_pos)\n",
    "\n",
    "#Sort the vocabulary by frequency\n",
    "sorted_voc = sorted(voc.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#Plot the Zipf distribution\n",
    "frequencies = [freq for _, freq in sorted_voc]\n",
    "rank = np.arange(len(frequencies))\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.title('Zipf distribution after stemming and POS filtering')\n",
    "plt.scatter(rank, frequencies, s=1)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Rank')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
